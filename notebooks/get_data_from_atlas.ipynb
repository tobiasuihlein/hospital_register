{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping bundes-klinik.atlas.de\n",
    "The purpose of this notebook is to web scrape data from the website and store it to CSV files for further processing. <br>\n",
    "Some code is served in functions. The web scraping process is basically divided into three parts:\n",
    "1) Opening the list with all hospitals available on the webiste\n",
    "2) Retrieving general details from the hospital pages\n",
    "3) Retrieving specific treatment details from the pages for these treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# load functions\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from lib.functions_webscrape_atlas import *\n",
    "\n",
    "# set display options\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare List of Hospital Ids\n",
    "### 1.1 Load Hospital Locations\n",
    "The list of all hospitals available on the website is loaded. <br>\n",
    "This information is necessary in order to target each hospital's individual pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read hospital locations data\n",
    "df_locations = pd.read_json('../data/in/raw/atlas/locations.json', dtype={'zip': str})\n",
    "\n",
    "# create list of hospital ids\n",
    "hospital_id_list = list(df_locations.copy()['link'].apply(lambda x: x.split('/')[-2]).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Clean Hospital Locations and add Column for Hospital Id\n",
    "First create column for hostpital id, then clean empty cells and save the data to CSV file for possible further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column for hospital_id\n",
    "df_locations['hospital_id'] = pd.DataFrame(df_locations.copy()['link'].apply(lambda x: x.split('/')[-2])).rename(columns={'link': 'hospital_id'})\n",
    "# replace empty cells with 'not reported'\n",
    "df_locations = df_locations.replace('', 'not reported')\n",
    "# save locations preprocessed data\n",
    "df_locations.to_csv('../data/in/staging/hospital_locations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Webscrape Departments, Certificates and other Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773870 - 100.0 %\n",
      "no certificates found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize lists to store departments data\n",
    "hospital_ids_departments = []\n",
    "department_names = []\n",
    "department_counts = []\n",
    "\n",
    "# initialize lists to store certificates data\n",
    "hospital_ids_certificates = []\n",
    "certificates = []\n",
    "\n",
    "# initialize lists to store details data\n",
    "hospital_ids_details = []\n",
    "total_treatments_counts = []\n",
    "total_treatments_labels = []\n",
    "nursing_quotient_counts = []\n",
    "nursing_quotient_labels = []\n",
    "nursing_counts = []\n",
    "provider_types = []\n",
    "bed_counts = []\n",
    "semi_residential_counts = []\n",
    "emergency_services = []\n",
    "\n",
    "# loop through all hospital ids\n",
    "k = 0\n",
    "for hospital_id in hospital_id_list:\n",
    "    k += 1\n",
    "    print(f'{hospital_id} - {round(k/len(hospital_id_list)*100, 1)} %')\n",
    "    \n",
    "    # load website content for specific hospital\n",
    "    soup = load_hospital_site(hospital_id)\n",
    "\n",
    "    # extract department data from website content\n",
    "    hospital_ids_, department_names_, department_counts_ = get_departments(soup, hospital_id)\n",
    "\n",
    "    hospital_ids_departments.extend(hospital_ids_)\n",
    "    department_names.extend(department_names_)\n",
    "    department_counts.extend(department_counts_)\n",
    "\n",
    "    # extract certificates data from website content\n",
    "    hospital_ids_certificates_, certificates_ = get_certificates(soup, hospital_id)\n",
    "\n",
    "    hospital_ids_certificates.extend(hospital_ids_certificates_)\n",
    "    certificates.extend(certificates_)\n",
    "\n",
    "    # extract details data from website content\n",
    "    hospital_id, total_treatments_count, total_treatments_label, nursing_quotient_count, nursing_quotient_label, nursing_count, provider_type, bed_count, semi_residential_count, emergency_service = get_details(soup, hospital_id)\n",
    "\n",
    "    hospital_ids_details.append(hospital_id)\n",
    "    total_treatments_counts.append(total_treatments_count)\n",
    "    total_treatments_labels.append(total_treatments_label)\n",
    "    nursing_quotient_counts.append(nursing_quotient_count)\n",
    "    nursing_quotient_labels.append(nursing_quotient_label)\n",
    "    nursing_counts.append(nursing_count)\n",
    "    provider_types.append(provider_type)\n",
    "    bed_counts.append(bed_count)\n",
    "    semi_residential_counts.append(semi_residential_count)\n",
    "    emergency_services.append(emergency_service)\n",
    "\n",
    "    # wait 15 sec between requests as requested by robots.txt of the website\n",
    "    time.sleep(15)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "# create dataframes from lists\n",
    "df_departments = pd.DataFrame({'hospital_id': hospital_ids_departments, 'department_name': department_names, 'department_count': department_counts})\n",
    "df_certificates = pd.DataFrame({'hospital_id': hospital_ids_certificates, 'certificate': certificates})\n",
    "df_details = pd.DataFrame({'hospital_id': hospital_ids_details, 'total_treatments': total_treatments_counts, 'total_treatments_label': total_treatments_labels, 'nursing_quotient': nursing_quotient_counts, 'nursing_quotient_label': nursing_quotient_labels, 'nursing_count': nursing_counts ,'provider_type': provider_types, 'bed_count': bed_counts, 'semi_residential_count': semi_residential_counts, 'emergency_service': emergency_services})\n",
    "\n",
    "# save department and details data for further processing\n",
    "df_departments.to_csv('../data/in/staging/atlas_departments.csv', index=False, encoding='utf-8')\n",
    "df_certificates.to_csv('../data/in/staging/atlas_certificates.csv', index=False, encoding='utf-8')\n",
    "df_details.to_csv('../data/in/staging/atlas_details.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrape Specific Treatments Information\n",
    "First create a two treatments dictionaries serving two pruposes: first for (later) storing the treatment codes in the database, second to provide the necessary details for the web scraping process (e.g. different treatments have specific hashes in the URL). The first dictionary is directly saved in a CSV file. Then the web scraping process is conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dictionary with treatment names as keys and urls as values\n",
    "url_dict = get_url_dict()\n",
    "\n",
    "treatments_dictionary = {}\n",
    "treatments_dict_for_db = {}\n",
    "\n",
    "# Extract treatment code, searchlabel and cHash from urls and store them in a dictionary\n",
    "for key, value in url_dict.items():\n",
    "    treatment_name = key\n",
    "    treatment_code = value.split('treatmentcode%5D=')[1].split('&')[0]\n",
    "    treatment_searchlabel = value.split('searchlabel%5D=')[1].split('&')[0]\n",
    "    treatment_cHash = value.split('cHash=')[1]\n",
    "    treatments_dictionary[treatment_name] = {\n",
    "        'code': treatment_code,\n",
    "        'searchlabel': treatment_searchlabel,\n",
    "        'cHash': treatment_cHash}\n",
    "    treatments_dict_for_db[treatment_code] = treatment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save treatments dictionary to csv for database import\n",
    "treatments_dict_df = pd.DataFrame({'treatment_code': treatments_dict_for_db.keys(), 'treatment_name': treatments_dict_for_db.values()})\n",
    "treatments_dict_df.to_csv('../data/in/staging/treatments_dict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get treatments for all hospitals and save to csv in chunks\n",
    "m = 50\n",
    "for k in range(len(hospital_id_list)//m):\n",
    "    k+=11\n",
    "    print('k:', k)\n",
    "    print(f'{k*m}-{k*m+m-1}')\n",
    "    list_for_df_hospital_id, list_for_df_treatment_code, list_for_df_count_number, list_for_df_count_label = get_treatments(hospital_ids[k*m:k*m+m], treatments_dictionary)\n",
    "    df_treatments = pd.DataFrame({'hospital_id': list_for_df_hospital_id, 'treatment_code': list_for_df_treatment_code, 'count_number': list_for_df_count_number, 'count_label': list_for_df_count_label})\n",
    "    df_treatments.to_csv(f'../data/in/staging/treatments_chunks/atlas_treatments_sample_{k*m}-{k*m+m-1}.csv', index=False, encoding='utf-8')\n",
    "    print(f'saved file {k*m}-{k*m+m-1}')\n",
    "    del df_treatments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
